# Desenvolvimento da aplica√ß√£o [*'EmotePet API'*]

## üë®‚Äçüíª Projeto desenvolvido por: [Brenno Ostemberg](https://github.com/brenno-ostemberg), [Jos√© Pedro C√¢ndido L.P.](https://github.com/PeterYouseph), [Rafael Torres Nantes](https://github.com/rafael-torres-nantes) e [Sarah Baraldi](https://github.com/mbaraldi-sarah).

## √çndice

* [üìö Contextualiza√ß√£o do projeto](#-contextualiza√ß√£o-do-projeto)
* [üõ†Ô∏è Tecnologias/Ferramentas utilizadas](#%EF%B8%8F-tecnologiasferramentas-utilizadas)
* [üñ•Ô∏è Funcionamento do sistema](#%EF%B8%8F-funcionamento-do-sistema)
   * [üòÅ Parte 1 - Emo√ß√µes](#-parte-1---emo√ß√µes)
   * [üê∂ Parte 2 - Emo√ß√µes e Pets](#-parte-2---emo√ß√µes-e-pets)
   * [‚òÅÔ∏è Inserindo logs no Cloudwatch](#%EF%B8%8F-inserindo-logs-no-cloudwatch)
   * [üîÄ Fluxo da aplica√ß√£o](#em-resumo-o-fluxo-da-aplica√ß√£o-se-d√°-da-seguinte-forma)
* [üìÅ Estrutura do projeto](#-estrutura-do-projeto)
* [üìå Como executar o projeto](#-como-executar-o-projeto)
* [üïµÔ∏è Dificuldades Encontradas](#%EF%B8%8F-dificuldades-encontradas)

## üìö Contextualiza√ß√£o do projeto

O projeto tem o objetivo de criar uma API que receba imagens postadas no *AWS S3*, utilize o *Amazon Rekognition* identificar bichos e humanos, utilize o *Amazon Bedrock* para extrair dicas de como cuidar dos pets reconhecidos e grave os *logs* dos resultados utilizando *CloudWatch*.


## üõ†Ô∏è Tecnologias/Ferramentas utilizadas

[<img src="https://img.shields.io/badge/Visual_Studio_Code-007ACC?logo=visual-studio-code&logoColor=white">](https://code.visualstudio.com/)
[<img src="https://img.shields.io/badge/Git-232F3E?logo=git&logoColor=red">](https://git-scm.com/)
[<img src="https://img.shields.io/badge/GitHub-181717?logo=github&logoColor=violet">](https://github.com/)
[<img src="https://img.shields.io/badge/AWS-fda100?logo=amazon-aws&logoColor=white">](https://aws.amazon.com/pt/)
[<img src="https://img.shields.io/badge/AWS-CLI-232F3E?logo=amazon-aws&logoColor=white">](https://aws.amazon.com/pt/cli/)
[<img src="https://img.shields.io/badge/AWS-S3-dd2304?logo=amazon-aws&logoColor=white">](https://aws.amazon.com/pt/s3/)
[<img src="https://img.shields.io/badge/AWS-Cloudwatch-green?logo=amazon-aws&logoColor=white">](https://aws.amazon.com/pt/cloudwatch/)
[<img src="https://img.shields.io/badge/Amazon-Bedrock-01ac71?logo=amazon-aws&logoColor=white">](https://aws.amazon.com/pt/bedrock/)
[<img src="https://img.shields.io/badge/Amazon-Rekognition-blue?logo=amazon-aws&logoColor=white">](https://aws.amazon.com/pt/rekognition/)
[<img src="https://img.shields.io/badge/Serverless_Framework-ff5242?logo=amazon-aws&logoColor=white">](https://www.serverless.com)

#### Organiza√ß√£o do Time:

[<img src="https://img.shields.io/badge/Trello-0079BF?logo=trello&logoColor=white">](https://trello.com/)
[<img src="https://img.shields.io/badge/Teams-6264A7?logo=microsoft-teams&logoColor=white">](https://www.microsoft.com/pt-br/microsoft-teams/group-chat-software)

## üñ•Ô∏è Funcionamento do sistema

### üòÅ Parte 1 - Emo√ß√µes

Utilizando o *framework* **Serverless**, enviamos um *json* via **POST** √† rota `/v1/vision`, onde utilizamos o **Amazon Rekognition** para detectar as faces e emo√ß√µes. Al√©m disso, s√£o efetuados *logs* referentes a cada solita√ß√£o no **AWS Cloudwatch**.

#### Exemplo de entrada:

<!-- Terminar -->
```json 
{ 
   "bucket": "nomeBucket", 
   "imageName": "nomeFoto.jpg" 
} 
```

#### Exemplos de Sa√≠da:

1. Caso haja apenas uma face:

<!-- Terminar -->
```json 
{ 
   "url_to_image": "https://myphotos/test.jpg", 
   "created_image": "02-02-2023 17:00:00", 
   "faces": [ 
     { 
      "position": 
      { 
       "Height": 0.06333330273628235, 
       "Left": 0.1718519926071167, 
       "Top": 0.7366669774055481, 
       "Width": 0.11061699688434601 
      } 
      "classified_emotion": "HAPPY", 
      "classified_emotion_confidence": 99.92965151369571686 
     } 
   ] 
} 
```

2. Caso haja mais de uma face, o elemento *"faces": [{...}]* recebe mais de um objeto:

<!-- Terminar -->
```json
"faces": [ 
     { 
      "position": 
      { 
       "Height": 0.06333330273628235, 
       "Left": 0.1718519926071167, 
       "Top": 0.7366669774055481, 
       "Width": 0.11061699688434601 
      } 
      "classified_emotion": "HAPPY", 
      "classified_emotion_confidence": 99.92965151369571686 
     },
     { 
      "position": 
      { 
       "Height": 0.08333330273628235, 
       "Left": 0.3718519926071167, 
       "Top": 0.6366669774055481, 
       "Width": 0.21061699688434601 
      } 
      "classified_emotion": "HAPPY", 
      "classified_emotion_confidence": 98.92965151369571686 
     }
   ]
```

3. Caso **N√ÉO** haja faces, os elementos contidos em *"faces": [{...}]* recebem valor *NULL*:

<!-- Terminar -->
```json 
{ 
   "url_to_image": "https://myphotos/test.jpg", 
   "created_image": "02-02-2023 17:00:00", 
   "faces": [ 
     { 
      "position": 
      { 
       "Height": Null, 
       "Left": Null, 
       "Top": Null, 
       "Width": Null 
      } 
      "classified_emotion": Null, 
      "classified_emotion_confidence": Null 
     } 
] 
} 
``` 

#### Detectando elementos:

- **Detectando as *faces***: utilizamos a fun√ß√£o `detect_faces` da seguinte forma:

```py
response = self.rekognition.detect_faces(
   Image={
      'S3Object': {
         'Bucket': bucket,
         'Name': image_name
      }
   },
   Attributes=['ALL']
)
```

- **Detectando as *emo√ß√µes***: utilizamos a fun√ß√£o `detect_labels` da seguinte forma:

```py
response = self.rekognition.detect_labels(
   Image={
      'S3Object': {
         'Bucket': bucket,
         'Name': image_name
      }
   },
   MaxLabels=10,
   MinConfidence=80
)
```

### üê∂ Parte 2 - Emo√ß√µes e Pets

De maneira an√°loga √† Parte 1, desenvolvemos o sistema utilizamos o *framework* **Serverless** para enviarmos um *json* via **POST** √† rota `/v2/vision`, onde utilizamos o **Amazon Rekognition** para detectar os pets, suas emo√ß√µes e ra√ßas. Al√©m disso, utilizamos o **Amazon Bedrock** para receber **dicas** de cuidados para cada ra√ßa dos pets reconhecidos. Por fim, s√£o efetuados *logs* referentes a cada solita√ß√£o no **AWS Cloudwatch**.

#### Exemplo de entrada:

<!-- Terminar -->
```json 
{ 
   "bucket": "nomeBucket", 
   "imageName": "nomeFoto.jpg" 
} 
```

#### Exemplos de Sa√≠da:

<!-- Terminar -->
1. Caso haja **apenas um pet**:

```json 
{  
   "url_to_image": "https://mycatphotos/cat.jpg",  
   "created_image": "02-02-2023 17:00:00",  
   "pets": [
      {
      "labels": [  
      {  
         "Confidence": 96.59198760986328,  
         "Name": "Animal"  
      },  
      {  
         "Confidence": 96.59198760986328,  
         "Name": "Dog"  
      },  
      {  
         "Confidence": 96.59198760986328,  
         "Name": "Pet"  
      },  
      {  
         "Confidence": 96.59198760986328,  
         "Name": "Labrador"  
      }  
      ]  
      "Dicas:": " 
      Dicas sobre Labradores: 
      N√≠vel de Energia e Necessidades de Exerc√≠cios: Labradores s√£o de m√©dio n√≠vel de energia, necessitando de 40 minutos de exerc√≠cio por dia. 
      Temperamento e Comportamento: Inteligentes, en√©rgicos, d√≥ceis, e com forte desejo de trabalhar com pessoas. 
      Cuidados e Necessidades: Pelos curtos que precisam de poucos cuidados, mas devem ser penteados uma vez por semana para remover fios mortos e soltos. A alimenta√ß√£o deve ser adequada, ajustando a quantidade conforme o peso do c√£o. 
      Problemas de Sa√∫de Comuns: Displasia do cotovelo e coxofemoral, atrofia progressiva da retina (APR) e catarata heredit√°ria. 
      " 
      }
   ]
} 
```

2. Caso haja **uma pessoa e um pet**, retorna uma resposta como √† da Parte 1, e tamb√©m informa√ß√µes de pet, como no exemplo anterior:

<!-- Terminar -->
```json
{ 
   "url_to_image": "https://myphotos/test.jpg", 
   "created_image": "02-02-2023 17:00:00", 
   "faces": [ 
     { 
      "position": 
      { 
       "Height": 0.06333330273628235, 
       "Left": 0.1718519926071167, 
       "Top": 0.7366669774055481, 
       "Width": 0.11061699688434601 
      } 
      "classified_emotion": "HAPPY", 
      "classified_emotion_confidence": 99.92965151369571686 
     } 
   ],
   "pets": [
      {
      "labels": [  
      {  
         "Confidence": 96.59198760986328,  
         "Name": "Animal"  
      },  
      {  
         "Confidence": 96.59198760986328,  
         "Name": "Dog"  
      },  
      {  
         "Confidence": 96.59198760986328,  
         "Name": "Pet"  
      },  
      {  
         "Confidence": 96.59198760986328,  
         "Name": "Labrador"  
      }  
      ]  
      "Dicas:": " 
      Dicas sobre Labradores: 
      N√≠vel de Energia e Necessidades de Exerc√≠cios: Labradores s√£o de m√©dio n√≠vel de energia, necessitando de 40 minutos de exerc√≠cio por dia. 
      Temperamento e Comportamento: Inteligentes, en√©rgicos, d√≥ceis, e com forte desejo de trabalhar com pessoas. 
      Cuidados e Necessidades: Pelos curtos que precisam de poucos cuidados, mas devem ser penteados uma vez por semana para remover fios mortos e soltos. A alimenta√ß√£o deve ser adequada, ajustando a quantidade conforme o peso do c√£o. 
      Problemas de Sa√∫de Comuns: Displasia do cotovelo e coxofemoral, atrofia progressiva da retina (APR) e catarata heredit√°ria. 
      " 
      }
   ]
}
```

#### Detectando elementos:

- **Detectando os *pets***: utilizamos a fun√ß√£o `detect_labels`:

```py
def detect_pets(self, bucket, image_name):
   labels_response = self.detect_labels(bucket, image_name)
   animal_characteristics = self.animal_characteristics(labels_response)
   pets = self.pets_labels_treatment(animal_characteristics)
   return pets
```

- Filtramos a resposta da fun√ß√£o para obtermos os **pets**:

```py
def animal_characteristics(self, rekognition_response):
   # Lista de r√≥tulos de poss√≠veis animais de estima√ß√£o
   pet_labels = ['Dog', 'Cat', 'Bird', 'Fish', 'Reptile', 'Mammal', 'Pet']
   animal_characteristics = []
   for label in rekognition_response["Labels"]:
      for parent in label["Parents"]:
            if parent.get("Name") in pet_labels:
               animal_characteristics.append(label)
   return animal_characteristics
```

- Para obtermos mais **precis√£o** no reconhecimento das ra√ßas dos animais, filtramos "ra√ßas gen√©ricas" do elemento ***breed***.

```py
def _process_pets(self, pet_labels):
   pets = []
   filtered_breeds = ['Animal', 'Pet', 'Dog', 'Bird', 'Mammal', 'Vertebrate', 'Canidae','Canine', 'Carnivore', 'Terrestrial animal', 'Dog breed', 'Dog like mammal']
   unique_breeds = {breed['Name']: breed for pet in pet_labels["pets"] for breed in pet["labels"] if breed['Name'] not in filtered_breeds}.values()

   # Gera o log das ra√ßas √∫nicas detectadas pelo Rekognition
   logger(f'Ra√ßas √∫nicas: {unique_breeds}')

   # Para cada ra√ßa de animal de estima√ß√£o detectada pelo Rekognition
   for breed in unique_breeds:
      self.bedrock_service.set_pet_breed(breed['Name'])
      response = self.bedrock_service.invoke_model()
      if response['statusCode'] == 200:
            tips = json.loads(response['Dicas'])
      else:
            tips = 'Erro ao obter dicas do Bedrock'

      pet_info = {
            'labels': [{'Confidence': breed['Confidence'], 'Name': breed['Name']}],
            'Dicas': tips
      }
      pets.append(pet_info)

   return pets
```

### ‚òÅÔ∏è Inserindo logs no Cloudwatch

Tanto na Parte 1 quanto na Parte 2, inserimos *logs* no **Cloudwatch**. Os *logs* foram formatados da seguinte maneira:

```py
log_event = {
   'logGroupName': log_group_name,
   'logStreamName': log_stream_name,
   'logEvents': [
      {
         'timestamp': int(datetime.datetime.now().timestamp() * 1000),
         'message': json.dumps(message)
      }
   ]
}
```

Criamos duas classifica√ß√µes de *logs*.

1. Quando a requisi√ß√£o for um sucesso:

```py
def logger(message):
    print(message)
    logger_instance.log_message('rekognition-logs', 'vision-logs', message)
```

2. Quando houver algum erro:

```py
def error(message):
    print(message)
    logger_instance.log_message('rekognition-logs', 'vision-errors', message)
```

### Em resumo, o fluxo da aplica√ß√£o se d√° da seguinte forma:

![Fluxo da Aplica√ß√£o](./assets/arquitetura-base.jpg)

## üìÅ Estrutura do projeto 

#### O projeto foi dividido nos seguintes diret√≥rios, baseando-se no modelo MVC (Model-View-Controller) com devidas adapta√ß√µes:

#### Divis√£o dos diret√≥rios:

- `controller` ‚Üí Realiza a chamada dos *services* (em ./services) criados para gerenciar os servi√ßos AWS, sendo *bucket* na **S3**, reconhecimento no **Amazon Rekognition** e cria√ß√£o de texto no **Amazon Bedrock**.

- `services` ‚Üí Manipulam os servi√ßos AWS obtendo os **metadados** e gera **URL** das imagens no **S3**, detecta faces e r√≥tulos no **Amazon Rekognition**, cria prompt e obtem respostas no **Amazon Bedrock**.

- `utils` ‚Üí Manipula os ***logs*** no **Cloudwatch** e faz *upload* de imagens no **S3**.

#### Outros arquivos importantes:

- `handler.py` ‚Üí Cont√©m as fun√ß√µes que sintetizam a API e define suas rotas. Verifica a sa√∫de da API, recebe a imagem do **S3** e retorna os detalhes do reconhecimento do **Amazon Rekognition**.

- `serverless.yml` ‚Üí Define as pol√≠ticas **IAM** para permitir que as **fun√ß√µes Lambda** acessem os servi√ßos necess√°rios e rotas das requisi√ß√µes que ser√£o usadas no *handler.py*. 

## üìå Como executar o projeto

### Clone o reposit√≥rio

```bash
$ git clone https://github.com/Compass-pb-aws-2024-MARCO/sprint-8-pb-aws-marco.git 
```

### Acesse a pasta do projeto no terminal/cmd:

```bash
$ cd sprints-8-pb-aws-marco
```

### Realize um check-out para a branch de desenvolvimento:

```bash
$ git checkout grupo-2
```

### Cerfitique-se ue tem o serverless instalado:

```bash
$ serverless
```

### Caso n√£o estiver, instale poe meio do comando:

```bash
$ npm install -g serverless
```

### Instale os plugins do serverless:

```bash
$ npm install serverless-python-requirements serverless-dotenv-plugin
```

### Configure as credenciais da aws:

```bash
$ aws configure
```

### Fa√ßa login no serverless:

```bash
$ serverless login
```

### Acesse a pasta visao-computacional:

```bash
$ cd visao-computacional
```

### Execute o seguinte comando para realizar o deploy:
```bash
$ serverless deploy
```

<!-- Terminar -->

## üïµÔ∏è Dificuldades Encontradas

### ‚öô Dificuldades T√©cnicas

O Rekognition retorna diversos dados, incluindo informa√ß√µes que **n√£o eram necess√°rias** para nosso uso, referidas como "ra√ßas gen√©ricas". Conseguimos solucionar o problema [aplicando filtros](#detectando-elementos-1) aos dados. No entanto, essa solu√ß√£o s√≥ foi alcan√ßada ap√≥s a an√°lise de v√°rios exemplos de retorno e uma **extensa** pesquisa na documenta√ß√£o do Rekognition sobre o t√≥pico, especialmente na [API Vision V2 (Emo√ß√µes + Pets)](#parte-2---emo√ß√µes-e-pets).

Outra dificuldade que enfrentamos foi o **timeout** ao configurarmos o *Bedrock*. O API Gateway possui um limite de 30 segundos para requisi√ß√µes HTTP, enquanto o *Bedrock* levava quase **5 minutos** para retornar os dados, mesmo para apenas um pet. Resolvemos esse problema realizando algumas modifica√ß√µes no c√≥digo, ajustando os atributos **maxTokenCount**, **temperature** e **topP** no arquivo `bedrock_services.py`.

```py
def generate_request_body(self):
   request_body = {
      "inputText": self.create_prompt(),
      "textGenerationConfig": {
            "maxTokenCount": 128,
            # temperature:  aleatoriedade na gera√ß√£o de texto
            "temperature": 0.7,
            # topP:  tokens que comp√µem o top p% da probabilidade cumulativa
            "topP": 0.9
      },
   }
   return json.dumps(request_body)
```

### üìù Dificuldades de Organiza√ß√£o

<!-- ... -->
